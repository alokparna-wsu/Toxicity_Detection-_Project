{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re, string\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "import xgboost as xgb\n",
    "import gc\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import scipy\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtypes = {\n",
    "    'comment_text'   : np.unicode ,\n",
    "    'toxic':         np.int16, \n",
    "    'severe_toxic': np.int16,\n",
    "    'obscene': np.int16,\n",
    "    'threat': np.int16,\n",
    "    'insult': np.int16,\n",
    "    'identity_hate': np.int16\n",
    "}\n",
    "\n",
    "train = pd.read_csv('C:/Users/abandyop/Desktop/Personal/Data Mining/Project/Data17/train.csv', dtype=dtypes, encoding='utf-8')\n",
    "test = pd.read_csv('C:/Users/abandyop/Desktop/Personal/Data Mining/Project/Data17/test.csv', dtype=dtypes, encoding='utf-8')\n",
    "\n",
    "train.comment_text.fillna(\"unknown\", inplace=True)\n",
    "test.comment_text.fillna(\"unknown\",  inplace=True)\n",
    "\n",
    "subm = pd.read_csv('C:/Users/abandyop/Desktop/Personal/Data Mining/Project/Data17/sample_submission.csv')    \n",
    "submid = pd.DataFrame({'id': subm[\"id\"].values.astype(str)}, dtype=np.str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization\n",
    "\n",
    "re_tok = re.compile(u'([{string.punctuation}“”¨«»®´·º½¾¿¡§£₤‘’])')\n",
    "\n",
    "def tokenize(s):\n",
    "    return re_tok.sub(r' \\1 ', s).split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mes, valid_mes, train_l, valid_l = train_test_split(train['comment_text'],\n",
    "                                                          train[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']],\n",
    "                                                          test_size=0.2, random_state=42)\n",
    "train_mes = pd.DataFrame(train_mes)\n",
    "valid_mes = pd.DataFrame(valid_mes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1999996it [07:35, 4387.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1999996 word vectors.\n"
     ]
    }
   ],
   "source": [
    "# load the FastText vectors in a dictionary:\n",
    "from tqdm import tqdm\n",
    "\n",
    "embeddings_index_FastText = {}\n",
    "f = open('C:/Users/abandyop/Desktop/Personal/Data Mining/Project/Datasets/crawl-300d-2M.vec', encoding='utf-8')\n",
    "for line in tqdm(f):\n",
    "    values = line.split()\n",
    "    word = values[0jhng .]\n",
    "    try:\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "    except:\n",
    "        continue\n",
    "    embeddings_index_FastText[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index_FastText))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create a normalized vector for the whole sentence\n",
    "def sent2vec(s, embeddings_index):\n",
    "    words = str(s).lower()\n",
    "    words = tokenize(words)\n",
    "    words = [w for w in words if w.isalpha()]\n",
    "    M = []\n",
    "    for w in words:\n",
    "        try:\n",
    "            M.append(embeddings_index[w])\n",
    "        except:\n",
    "            continue\n",
    "    M = np.array(M)\n",
    "    v = M.sum(axis=0)\n",
    "    if type(v) != np.ndarray:\n",
    "        return np.zeros(300)\n",
    "    return v / np.sqrt((v ** 2).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xtrain_FastText.shape =  (127656, 300)\n",
      "xvalid_FastText.shape =  (31915, 300)\n",
      "xtest_FastText.shape =  (153164, 300)\n"
     ]
    }
   ],
   "source": [
    "gc.collect()\n",
    "\n",
    "# Create sentence vectors using the above function for training and validation set\n",
    "xtrain_FastText = [sent2vec(x, embeddings_index_FastText) for x in train_mes['comment_text']]\n",
    "xvalid_FastText = [sent2vec(x, embeddings_index_FastText) for x in valid_mes['comment_text']]\n",
    "\n",
    "xtrain_FastText = np.array(xtrain_FastText)\n",
    "xvalid_FastText = np.array(xvalid_FastText)\n",
    "\n",
    "print(\"xtrain_FastText.shape = \", xtrain_FastText.shape)\n",
    "print(\"xvalid_FastText.shape = \", xvalid_FastText.shape)\n",
    " \n",
    "# Generate Word vectors of test data\n",
    "xtest_FastText = [sent2vec(x, embeddings_index_FastText) for x in test['comment_text']]\n",
    "xtest_FastText = np.array(xtest_FastText)\n",
    "\n",
    "print(\"xtest_FastText.shape = \", xtest_FastText.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit toxic\n",
      "CV score for class toxic is 0.7967549344738636\n",
      "Validation score for class toxic is 0.758765470781764\n"
     ]
    }
   ],
   "source": [
    "# col = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "col = ['toxic']\n",
    "preds = np.zeros((valid_mes.shape[0], len(col))).astype(object)\n",
    "\n",
    "for i, class_name in enumerate(col):\n",
    "    print('fit '+ class_name)\n",
    "    classifier = SGDClassifier(loss='log', max_iter=1000, epsilon=0.001, n_jobs=-1, class_weight='balanced')\n",
    "\n",
    "    cv_score = np.mean(cross_val_score(classifier, xtrain_FastText, train_l[class_name], cv=5, scoring='roc_auc'))\n",
    "    print('CV score for class {} is {}'.format(class_name, cv_score))\n",
    "\n",
    "    classifier.fit(xtrain_FastText, train_l[class_name])\n",
    "    \n",
    "    val_score = classifier.score(xvalid_FastText, valid_l[class_name])\n",
    "    print('Validation score for class {} is {}'.format(class_name, val_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit toxic\n",
      "CV score for class toxic is 0.7967549344738636\n",
      "Validation score for class toxic is 0.758765470781764\n"
     ]
    }
   ],
   "source": [
    "# col = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "col = ['toxic']\n",
    "preds = np.zeros((valid_mes.shape[0], len(col))).astype(object)\n",
    "\n",
    "for i, class_name in enumerate(col):\n",
    "    print('fit '+ class_name)\n",
    "    classifier = SGDClassifier(loss='log', max_iter=1000, epsilon=0.001, n_jobs=-1, class_weight='balanced')\n",
    "\n",
    "    cv_score = np.mean(cross_val_score(classifier, xtrain_FastText, train_l[class_name], cv=5, scoring='roc_auc'))\n",
    "    print('CV score for class {} is {}'.format(class_name, cv_score))\n",
    "\n",
    "    classifier.fit(xtrain_FastText, train_l[class_name])\n",
    "    \n",
    "    val_score = classifier.score(xvalid_FastText, valid_l[class_name])\n",
    "    print('Validation score for class {} is {}'.format(class_name, val_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit toxic\n",
      "CV score for class toxic is 0.7969993287759112\n",
      "Validation score for class toxic is 0.7860567131442895\n"
     ]
    }
   ],
   "source": [
    "# col = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "col = ['toxic']\n",
    "preds = np.zeros((valid_mes.shape[0], len(col))).astype(object)\n",
    "\n",
    "for i, class_name in enumerate(col):\n",
    "    print('fit '+ class_name)\n",
    "    classifier = LogisticRegression(C=0.1, solver='sag', class_weight='balanced', max_iter=1000)\n",
    "    \n",
    "    cv_score = np.mean(cross_val_score(classifier, xtrain_FastText, train_l[class_name], cv=5, scoring='roc_auc'))\n",
    "    print('CV score for class {} is {}'.format(class_name, cv_score))\n",
    "\n",
    "    classifier.fit(xtrain_FastText, train_l[class_name])\n",
    "    \n",
    "    val_score = classifier.score(xvalid_FastText, valid_l[class_name])\n",
    "    print('Validation score for class {} is {}'.format(class_name, val_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runXGB(train_X, train_y, test_X, test_y=None, feature_names=None, seed_val=2017, num_rounds=500):\n",
    "    param = {}\n",
    "    param['objective'] = 'binary:logistic'\n",
    "    param['eta'] = 0.1\n",
    "    param['max_depth'] = 6\n",
    "    param['silent'] = 1\n",
    "    param['eval_metric'] = 'auc'\n",
    "    param['min_child_weight'] = 1\n",
    "    param['subsample'] = 0.7\n",
    "    param['colsample_bytree'] = 0.7\n",
    "    param['seed'] = seed_val\n",
    "    num_rounds = num_rounds\n",
    "\n",
    "    plst = list(param.items())\n",
    "    xgtrain = xgb.DMatrix(train_X, label=train_y)\n",
    "\n",
    "    if test_y is not None:\n",
    "        xgtest = xgb.DMatrix(test_X, label=test_y)\n",
    "        watchlist = [ (xgtrain,'train'), (xgtest, 'valid') ]\n",
    "        model = xgb.train(plst, xgtrain, num_rounds, watchlist, early_stopping_rounds=20)\n",
    "    else:\n",
    "        xgtest = xgb.DMatrix(test_X)\n",
    "        model = xgb.train(plst, xgtrain, num_rounds)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit toxic\n",
      "[0]\ttrain-auc:0.77959\tvalid-auc:0.76944\n",
      "Multiple eval metrics have been passed: 'valid-auc' will be used for early stopping.\n",
      "\n",
      "Will train until valid-auc hasn't improved in 20 rounds.\n",
      "[1]\ttrain-auc:0.79937\tvalid-auc:0.78615\n",
      "[2]\ttrain-auc:0.80626\tvalid-auc:0.79430\n",
      "[3]\ttrain-auc:0.81165\tvalid-auc:0.79850\n",
      "[4]\ttrain-auc:0.81605\tvalid-auc:0.80091\n",
      "[5]\ttrain-auc:0.81868\tvalid-auc:0.80310\n",
      "[6]\ttrain-auc:0.82023\tvalid-auc:0.80477\n",
      "[7]\ttrain-auc:0.82232\tvalid-auc:0.80581\n",
      "[8]\ttrain-auc:0.82499\tvalid-auc:0.80657\n",
      "[9]\ttrain-auc:0.82614\tvalid-auc:0.80772\n",
      "[10]\ttrain-auc:0.82742\tvalid-auc:0.80840\n",
      "[11]\ttrain-auc:0.82886\tvalid-auc:0.80893\n",
      "[12]\ttrain-auc:0.83044\tvalid-auc:0.80936\n",
      "[13]\ttrain-auc:0.83203\tvalid-auc:0.81109\n",
      "[14]\ttrain-auc:0.83349\tvalid-auc:0.81186\n",
      "[15]\ttrain-auc:0.83542\tvalid-auc:0.81312\n",
      "[16]\ttrain-auc:0.83679\tvalid-auc:0.81397\n",
      "[17]\ttrain-auc:0.83806\tvalid-auc:0.81423\n",
      "[18]\ttrain-auc:0.83949\tvalid-auc:0.81507\n",
      "[19]\ttrain-auc:0.84084\tvalid-auc:0.81635\n",
      "[20]\ttrain-auc:0.84191\tvalid-auc:0.81658\n",
      "[21]\ttrain-auc:0.84280\tvalid-auc:0.81688\n",
      "[22]\ttrain-auc:0.84410\tvalid-auc:0.81766\n",
      "[23]\ttrain-auc:0.84555\tvalid-auc:0.81825\n",
      "[24]\ttrain-auc:0.84695\tvalid-auc:0.81863\n",
      "[25]\ttrain-auc:0.84822\tvalid-auc:0.81970\n",
      "[26]\ttrain-auc:0.84949\tvalid-auc:0.82008\n",
      "[27]\ttrain-auc:0.85064\tvalid-auc:0.82058\n",
      "[28]\ttrain-auc:0.85187\tvalid-auc:0.82134\n",
      "[29]\ttrain-auc:0.85327\tvalid-auc:0.82185\n",
      "[30]\ttrain-auc:0.85450\tvalid-auc:0.82256\n",
      "[31]\ttrain-auc:0.85567\tvalid-auc:0.82347\n",
      "[32]\ttrain-auc:0.85688\tvalid-auc:0.82346\n",
      "[33]\ttrain-auc:0.85814\tvalid-auc:0.82438\n",
      "[34]\ttrain-auc:0.85941\tvalid-auc:0.82501\n",
      "[35]\ttrain-auc:0.86073\tvalid-auc:0.82592\n",
      "[36]\ttrain-auc:0.86198\tvalid-auc:0.82655\n",
      "[37]\ttrain-auc:0.86306\tvalid-auc:0.82726\n",
      "[38]\ttrain-auc:0.86411\tvalid-auc:0.82806\n",
      "[39]\ttrain-auc:0.86539\tvalid-auc:0.82876\n",
      "[40]\ttrain-auc:0.86652\tvalid-auc:0.82936\n",
      "[41]\ttrain-auc:0.86782\tvalid-auc:0.82991\n",
      "[42]\ttrain-auc:0.86891\tvalid-auc:0.83005\n",
      "[43]\ttrain-auc:0.87016\tvalid-auc:0.83085\n",
      "[44]\ttrain-auc:0.87109\tvalid-auc:0.83138\n",
      "[45]\ttrain-auc:0.87225\tvalid-auc:0.83189\n",
      "[46]\ttrain-auc:0.87327\tvalid-auc:0.83233\n",
      "[47]\ttrain-auc:0.87446\tvalid-auc:0.83245\n",
      "[48]\ttrain-auc:0.87558\tvalid-auc:0.83288\n",
      "[49]\ttrain-auc:0.87655\tvalid-auc:0.83322\n",
      "[50]\ttrain-auc:0.87762\tvalid-auc:0.83374\n",
      "[51]\ttrain-auc:0.87862\tvalid-auc:0.83405\n",
      "[52]\ttrain-auc:0.87956\tvalid-auc:0.83466\n",
      "[53]\ttrain-auc:0.88046\tvalid-auc:0.83509\n",
      "[54]\ttrain-auc:0.88151\tvalid-auc:0.83550\n",
      "[55]\ttrain-auc:0.88236\tvalid-auc:0.83570\n",
      "[56]\ttrain-auc:0.88330\tvalid-auc:0.83633\n",
      "[57]\ttrain-auc:0.88424\tvalid-auc:0.83668\n",
      "[58]\ttrain-auc:0.88512\tvalid-auc:0.83714\n",
      "[59]\ttrain-auc:0.88564\tvalid-auc:0.83724\n",
      "[60]\ttrain-auc:0.88650\tvalid-auc:0.83760\n",
      "[61]\ttrain-auc:0.88743\tvalid-auc:0.83803\n",
      "[62]\ttrain-auc:0.88841\tvalid-auc:0.83827\n",
      "[63]\ttrain-auc:0.88930\tvalid-auc:0.83880\n",
      "[64]\ttrain-auc:0.88973\tvalid-auc:0.83893\n",
      "[65]\ttrain-auc:0.89051\tvalid-auc:0.83900\n",
      "[66]\ttrain-auc:0.89135\tvalid-auc:0.83914\n",
      "[67]\ttrain-auc:0.89203\tvalid-auc:0.83930\n",
      "[68]\ttrain-auc:0.89260\tvalid-auc:0.83962\n",
      "[69]\ttrain-auc:0.89312\tvalid-auc:0.83984\n",
      "[70]\ttrain-auc:0.89384\tvalid-auc:0.83997\n",
      "[71]\ttrain-auc:0.89446\tvalid-auc:0.84021\n",
      "[72]\ttrain-auc:0.89506\tvalid-auc:0.84033\n",
      "[73]\ttrain-auc:0.89546\tvalid-auc:0.84040\n",
      "[74]\ttrain-auc:0.89623\tvalid-auc:0.84062\n",
      "[75]\ttrain-auc:0.89712\tvalid-auc:0.84088\n",
      "[76]\ttrain-auc:0.89782\tvalid-auc:0.84103\n",
      "[77]\ttrain-auc:0.89869\tvalid-auc:0.84124\n",
      "[78]\ttrain-auc:0.89939\tvalid-auc:0.84140\n",
      "[79]\ttrain-auc:0.90002\tvalid-auc:0.84155\n",
      "[80]\ttrain-auc:0.90087\tvalid-auc:0.84167\n",
      "[81]\ttrain-auc:0.90151\tvalid-auc:0.84199\n",
      "[82]\ttrain-auc:0.90209\tvalid-auc:0.84219\n",
      "[83]\ttrain-auc:0.90250\tvalid-auc:0.84236\n",
      "[84]\ttrain-auc:0.90279\tvalid-auc:0.84244\n",
      "[85]\ttrain-auc:0.90331\tvalid-auc:0.84253\n",
      "[86]\ttrain-auc:0.90402\tvalid-auc:0.84267\n",
      "[87]\ttrain-auc:0.90478\tvalid-auc:0.84277\n",
      "[88]\ttrain-auc:0.90531\tvalid-auc:0.84294\n",
      "[89]\ttrain-auc:0.90585\tvalid-auc:0.84325\n",
      "[90]\ttrain-auc:0.90648\tvalid-auc:0.84348\n",
      "[91]\ttrain-auc:0.90688\tvalid-auc:0.84349\n",
      "[92]\ttrain-auc:0.90743\tvalid-auc:0.84388\n",
      "[93]\ttrain-auc:0.90799\tvalid-auc:0.84404\n",
      "[94]\ttrain-auc:0.90831\tvalid-auc:0.84423\n",
      "[95]\ttrain-auc:0.90876\tvalid-auc:0.84433\n",
      "[96]\ttrain-auc:0.90942\tvalid-auc:0.84438\n",
      "[97]\ttrain-auc:0.91000\tvalid-auc:0.84442\n",
      "[98]\ttrain-auc:0.91062\tvalid-auc:0.84448\n",
      "[99]\ttrain-auc:0.91124\tvalid-auc:0.84464\n",
      "[100]\ttrain-auc:0.91183\tvalid-auc:0.84475\n",
      "[101]\ttrain-auc:0.91249\tvalid-auc:0.84482\n",
      "[102]\ttrain-auc:0.91293\tvalid-auc:0.84495\n",
      "[103]\ttrain-auc:0.91354\tvalid-auc:0.84511\n",
      "[104]\ttrain-auc:0.91418\tvalid-auc:0.84538\n",
      "[105]\ttrain-auc:0.91472\tvalid-auc:0.84547\n",
      "[106]\ttrain-auc:0.91546\tvalid-auc:0.84557\n",
      "[107]\ttrain-auc:0.91571\tvalid-auc:0.84564\n",
      "[108]\ttrain-auc:0.91607\tvalid-auc:0.84571\n",
      "[109]\ttrain-auc:0.91651\tvalid-auc:0.84584\n",
      "[110]\ttrain-auc:0.91712\tvalid-auc:0.84603\n",
      "[111]\ttrain-auc:0.91765\tvalid-auc:0.84594\n",
      "[112]\ttrain-auc:0.91796\tvalid-auc:0.84581\n",
      "[113]\ttrain-auc:0.91844\tvalid-auc:0.84575\n",
      "[114]\ttrain-auc:0.91890\tvalid-auc:0.84589\n",
      "[115]\ttrain-auc:0.91945\tvalid-auc:0.84584\n",
      "[116]\ttrain-auc:0.91977\tvalid-auc:0.84587\n",
      "[117]\ttrain-auc:0.92031\tvalid-auc:0.84581\n",
      "[118]\ttrain-auc:0.92058\tvalid-auc:0.84592\n",
      "[119]\ttrain-auc:0.92105\tvalid-auc:0.84597\n",
      "[120]\ttrain-auc:0.92161\tvalid-auc:0.84613\n",
      "[121]\ttrain-auc:0.92217\tvalid-auc:0.84618\n",
      "[122]\ttrain-auc:0.92277\tvalid-auc:0.84633\n",
      "[123]\ttrain-auc:0.92327\tvalid-auc:0.84655\n",
      "[124]\ttrain-auc:0.92371\tvalid-auc:0.84670\n",
      "[125]\ttrain-auc:0.92419\tvalid-auc:0.84688\n",
      "[126]\ttrain-auc:0.92453\tvalid-auc:0.84678\n",
      "[127]\ttrain-auc:0.92498\tvalid-auc:0.84671\n",
      "[128]\ttrain-auc:0.92513\tvalid-auc:0.84665\n",
      "[129]\ttrain-auc:0.92573\tvalid-auc:0.84684\n",
      "[130]\ttrain-auc:0.92617\tvalid-auc:0.84687\n",
      "[131]\ttrain-auc:0.92683\tvalid-auc:0.84701\n",
      "[132]\ttrain-auc:0.92731\tvalid-auc:0.84700\n",
      "[133]\ttrain-auc:0.92779\tvalid-auc:0.84720\n",
      "[134]\ttrain-auc:0.92800\tvalid-auc:0.84728\n",
      "[135]\ttrain-auc:0.92846\tvalid-auc:0.84725\n",
      "[136]\ttrain-auc:0.92891\tvalid-auc:0.84730\n",
      "[137]\ttrain-auc:0.92926\tvalid-auc:0.84738\n",
      "[138]\ttrain-auc:0.92949\tvalid-auc:0.84718\n",
      "[139]\ttrain-auc:0.92992\tvalid-auc:0.84734\n",
      "[140]\ttrain-auc:0.93020\tvalid-auc:0.84741\n",
      "[141]\ttrain-auc:0.93049\tvalid-auc:0.84753\n",
      "[142]\ttrain-auc:0.93119\tvalid-auc:0.84765\n",
      "[143]\ttrain-auc:0.93170\tvalid-auc:0.84766\n",
      "[144]\ttrain-auc:0.93214\tvalid-auc:0.84781\n",
      "[145]\ttrain-auc:0.93263\tvalid-auc:0.84779\n",
      "[146]\ttrain-auc:0.93286\tvalid-auc:0.84786\n",
      "[147]\ttrain-auc:0.93302\tvalid-auc:0.84789\n",
      "[148]\ttrain-auc:0.93349\tvalid-auc:0.84794\n",
      "[149]\ttrain-auc:0.93380\tvalid-auc:0.84799\n",
      "[150]\ttrain-auc:0.93426\tvalid-auc:0.84804\n",
      "[151]\ttrain-auc:0.93477\tvalid-auc:0.84819\n",
      "[152]\ttrain-auc:0.93515\tvalid-auc:0.84817\n",
      "[153]\ttrain-auc:0.93561\tvalid-auc:0.84811\n",
      "[154]\ttrain-auc:0.93603\tvalid-auc:0.84804\n",
      "[155]\ttrain-auc:0.93635\tvalid-auc:0.84809\n",
      "[156]\ttrain-auc:0.93659\tvalid-auc:0.84804\n",
      "[157]\ttrain-auc:0.93677\tvalid-auc:0.84810\n",
      "[158]\ttrain-auc:0.93703\tvalid-auc:0.84830\n",
      "[159]\ttrain-auc:0.93742\tvalid-auc:0.84824\n",
      "[160]\ttrain-auc:0.93798\tvalid-auc:0.84832\n",
      "[161]\ttrain-auc:0.93836\tvalid-auc:0.84849\n",
      "[162]\ttrain-auc:0.93865\tvalid-auc:0.84856\n",
      "[163]\ttrain-auc:0.93916\tvalid-auc:0.84865\n",
      "[164]\ttrain-auc:0.93948\tvalid-auc:0.84865\n",
      "[165]\ttrain-auc:0.93982\tvalid-auc:0.84876\n",
      "[166]\ttrain-auc:0.94025\tvalid-auc:0.84876\n",
      "[167]\ttrain-auc:0.94050\tvalid-auc:0.84882\n",
      "[168]\ttrain-auc:0.94070\tvalid-auc:0.84882\n",
      "[169]\ttrain-auc:0.94090\tvalid-auc:0.84886\n",
      "[170]\ttrain-auc:0.94123\tvalid-auc:0.84889\n",
      "[171]\ttrain-auc:0.94134\tvalid-auc:0.84889\n",
      "[172]\ttrain-auc:0.94167\tvalid-auc:0.84901\n",
      "[173]\ttrain-auc:0.94199\tvalid-auc:0.84902\n",
      "[174]\ttrain-auc:0.94228\tvalid-auc:0.84910\n",
      "[175]\ttrain-auc:0.94268\tvalid-auc:0.84912\n",
      "[176]\ttrain-auc:0.94281\tvalid-auc:0.84917\n",
      "[177]\ttrain-auc:0.94303\tvalid-auc:0.84918\n",
      "[178]\ttrain-auc:0.94350\tvalid-auc:0.84916\n",
      "[179]\ttrain-auc:0.94382\tvalid-auc:0.84916\n",
      "[180]\ttrain-auc:0.94439\tvalid-auc:0.84915\n",
      "[181]\ttrain-auc:0.94475\tvalid-auc:0.84928\n",
      "[182]\ttrain-auc:0.94497\tvalid-auc:0.84933\n",
      "[183]\ttrain-auc:0.94541\tvalid-auc:0.84948\n",
      "[184]\ttrain-auc:0.94572\tvalid-auc:0.84949\n",
      "[185]\ttrain-auc:0.94613\tvalid-auc:0.84953\n",
      "[186]\ttrain-auc:0.94632\tvalid-auc:0.84961\n",
      "[187]\ttrain-auc:0.94653\tvalid-auc:0.84964\n",
      "[188]\ttrain-auc:0.94686\tvalid-auc:0.84968\n",
      "[189]\ttrain-auc:0.94710\tvalid-auc:0.84969\n",
      "[190]\ttrain-auc:0.94722\tvalid-auc:0.84968\n",
      "[191]\ttrain-auc:0.94773\tvalid-auc:0.84958\n",
      "[192]\ttrain-auc:0.94812\tvalid-auc:0.84945\n",
      "[193]\ttrain-auc:0.94845\tvalid-auc:0.84940\n",
      "[194]\ttrain-auc:0.94878\tvalid-auc:0.84940\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[195]\ttrain-auc:0.94908\tvalid-auc:0.84937\n",
      "[196]\ttrain-auc:0.94937\tvalid-auc:0.84936\n",
      "[197]\ttrain-auc:0.94969\tvalid-auc:0.84937\n",
      "[198]\ttrain-auc:0.94994\tvalid-auc:0.84937\n",
      "[199]\ttrain-auc:0.95011\tvalid-auc:0.84934\n",
      "[200]\ttrain-auc:0.95038\tvalid-auc:0.84957\n",
      "[201]\ttrain-auc:0.95072\tvalid-auc:0.84949\n",
      "[202]\ttrain-auc:0.95110\tvalid-auc:0.84945\n",
      "[203]\ttrain-auc:0.95142\tvalid-auc:0.84944\n",
      "[204]\ttrain-auc:0.95153\tvalid-auc:0.84952\n",
      "[205]\ttrain-auc:0.95182\tvalid-auc:0.84953\n",
      "[206]\ttrain-auc:0.95218\tvalid-auc:0.84962\n",
      "[207]\ttrain-auc:0.95246\tvalid-auc:0.84968\n",
      "[208]\ttrain-auc:0.95281\tvalid-auc:0.84967\n",
      "[209]\ttrain-auc:0.95323\tvalid-auc:0.84962\n",
      "Stopping. Best iteration:\n",
      "[189]\ttrain-auc:0.94710\tvalid-auc:0.84969\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# col = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "col = ['toxic']\n",
    "preds = np.zeros((test.shape[0], len(col)))\n",
    "\n",
    "for i, j in enumerate(col):\n",
    "    print('fit '+j)\n",
    "    model = runXGB(xtrain_FastText, train_l[j], xvalid_FastText, valid_l[j])\n",
    "    preds[:,i] = model.predict(xgb.DMatrix(xtest_FastText), ntree_limit = model.best_ntree_limit)\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.concat([submid, pd.DataFrame(preds, columns = col)], axis=1)\n",
    "submission.to_csv('sample_submission_FastText_xgb.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
